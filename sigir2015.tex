\documentclass{sig-alternate}
%\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsmath,amsfonts,amssymb}
%\usepackage{subfigure}
%\usepackage{multirow}
\usepackage{url}
\usepackage{color}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\ignore}[1]{}

\begin{document}

\title{Information retrieval using temporally smoothed language models}

%\ignore{
\numberofauthors{2}
\author{
Craig Willis, Miles Efron\\[1ex]
\affaddr{Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign}\\
\affaddr{\{willis8, mefron\}@illinois.edu}
}
%}

\maketitle
\begin{abstract}

This paper presents a novel approach to utilizing temporal information to improve the effectiveness of ad hoc information retrieval (IR).  While temporally-informed IR is an active research area, previous work has focused mainly on improving ranking by analysis of the timestamps associated with retrieved documents.  In contrast, we integrate time more directly into retrieval via the smoothing of language models.  In our approach, document models are smoothed using  \emph{temporal language models}--language models corresponding to a period in time (e.g., day, week, or month) in a collection. We present three temporal smoothing methods and evaluate them using seven standard TREC collections. We show that document smoothing using temporal language models improves retrieval effectiveness over state-of-the art approaches. We demonstrate that this method complements established approaches that operate on document timestamp distributions.

\end{abstract}


%% section: introduction
\section{Introduction}\label{section.intro}

Temporality is an intrinsic characteristic of information systems. Documents are published at particular
points in time, users issue queries over time, collections have temporal constraints, languages and 
writing styles even change over time. Temporal constraints, such as date ranges and sort-by options, 
have long been standard features of search interfaces for decades.  However, even today general-purpose
retrieval algorithms do not directly incorporate temporal information. 

Over the past decade, researchers have explored the role of temporal information in a variety of 
retrieval contexts.  Studies have explored techniques for extracting temporal expressions \cite{Childs1999, Arikan2009, Alonso2012, Lin2014}, studied the temporal dynamics of language \cite{Michel2011a}, documents \cite{Teevan2009}, queries \cite{Jones2007, Shokouhi2011}, and users' behaviors \cite{Radinsky2012}.  Numerous models have been proposed incorporating temporal information into the retrieval process, generally relying on the analysis of timestamps associated with documents \cite{Li2003, Dakka2012, Efron2011, Efron2014, Peetz2013a}. Closer to the work presented in this paper, \cite{DeJong2005} used language models for temporal intervals to date historical texts.

In this paper we study how temporal information can be directly incorporated into the language modeling framework through the use of \emph{temporal language models}.  Similar to \cite{DeJong2005}, temporal language models are language models corresponding to a period in time (e.g., day, week, or month) in a document collection.  We propose that temporal language models can be used to smooth document language models, improving model estimation and retrieval effectiveness. This work can be seen as building on the temporal retrieval models of \cite{Li2003, Dakka2012, Efron2011, Efron2014} and the cluster- or LDA-based smoothing methods proposed by \cite{Liu2004} and \cite{Wei2006}.

To our knowledge, temporal language models have never been used in this way.  Earlier work in \cite{DeJong2005} used temporal language models to predict the date of historical documents.

\section{Related work}

\subsection{Temporal information retrieval}

Li and Croft \cite{Li2003} were the first to suggest that documents can be scored based on a combination of lexical and temporal information, which they refer to as ``time-based language models''. Working within the language modeling framework \cite{Ponte1998}, they incorporate temporal information via an exponential prior modeling the relevancy of documents based on timestamps. This later became known as a ``recency model'' because it heavily favors recent documents.

This basic model was later extended by  Dakka, Gravano, and Ipeirtos \cite{Dakka2012} to support any time-sensitive query. After an initial retrieval, a histogram is created from document timestamps and bins are re-ordered by descending magnitude. An exponential distribution is used to model the importance of documents in each bin, with a global rate parameter estimated for the collection.  Efron, and Golovchinsky \cite{Efron2011} further refine this model to allow for query-specific estimation of the exponential rate parameter.  Finally, Efron, Lin, He and de Vries \cite{Efron2014} replace the histogram approach with a kernel-density estimator (KDE) to estimate the query-specific temporal distribution of results.  Each of these methods depends on an initial retrieval to estimate the distribution of document timestamps.

In each of these models, temporal information is assumed to be independent of the lexical information. Standard query likelihood scores are combined with the temporal scores based on this assumption.

Efron and Golovchinsky \cite{Efron2011} also explore a novel model using temporal information to smooth the document language model. In this case, the smoothing parameter $\lambda_t$ in Jelinek-Mercer smoothing is learned based on the temporal distribution of results. The amount of smoothing for each document varies depending on the time of each document $d$.

\subsection{Temporally-estimated relevance models}

Each of the above models is also considered in the context of relevance models \cite{Lavrenko2001}. In each case, the temporal models are used to score documents in an initial retrieval, the results of which used to construct relevance models. Only \cite{Efron2014} re-scores the final results using the temporal model. Peetz, Meij, and Rijke \cite{Peetz2013a} propose an alternative query modeling approach that relies on the identification of temporal ``bursts'' in pseudo-feedback. 

\subsection{Document-specific smoothing}

As noted by Zhai \cite{Zhai2008}, the usual approach of smoothing all documents with the same collection language model seems suboptimal. Document-dependent smoothing methods, such as those proposed in \cite{Liu2004} and  \cite{Wei2006}, instead smooth the document language model with document-dependent models constructed using methods such as clustering or topic modeling. Liu and Croft \cite{Liu2004} use k-means to cluster documents and create cluster language models.  Wei and Croft \cite{Wei2006} use latent Dirichlet allocation (LDA) to calculate topic models. Both studies use a two-phase smoothing approach. The cluster or topic language models are smoothed using the collection language model.  The resulting model is then used to smooth the document language model.  Either Jelinek-Mercer or Dirichlet smoothing can be used in either phase, but both studies reported results using Jelinek-Mercer for the first phase and Dirichlet for the second. This general approach is adopted in the models described below.

\section{Temporal language models}

We define a \textit{temporal language model} as a unigram model of terms in a collection at time or interval $t$.  We can imagine that the language model for a collection for the year 1900 is different than a language model for 1950.  In a stream of news, the language model for 1995 might be different than the language model for 2000. From this, we can define a generative model of either queries or documents. Given some interval $t$, the probability that the interval generated the query (or document) can be estimated as $p(q \vert \theta_t)$. The interval could be an hour, day, week, or year, depending on the characteristics of the collection.

\subsection{Model 1: Independent evidence}

Following \cite{Dakka2012} we can directly score documents using the document and temporal language models treated independently: 

\[ 
p_{TD}(d \vert q) \propto p(q \vert \theta_d) \cdot p(d) \cdot p(q \vert \theta_t) \cdot p(t)
\]

Where $\theta_d$ is the document language model and $\theta_t$ is the temporal language model. We refer to this as Model 1.

\subsection{Model 2: Smoothing using the temporal language model based on document time}

Following \cite{Liu2004} and \cite{Wei2006}, we can smooth the document language model using the temporal language model and a two-phase smoothing approach:

\[
p_{TSM}(w \vert d) = \dfrac{n(w,D) + \mu \cdot \left[ (1-\alpha) \cdot p(w\vert \theta_t) + \alpha \cdot p(w \vert \theta_c)\right]}{n(D) + \mu}
\]

Where $\theta_t$ is the temporal language model for the document time $d_t$ and  $\theta_c$ is the collection language model. Under this model, temporal evidence is no longer treated as independent. The two-stage smoothing approach can use either Jelinek-Mercer or Dirichlet smoothing for either stage.  In their study of smoothing methods, \cite{Zhai2004} found that Dirichlet smoothing was more effective for explaining unseen words while Jelinek-Mercer was more effective for explaining noise in the query for longer queries.

\subsection{Model 3: Smoothing using the most likely temporal language model}

Loosening the restriction that a document was generated based on the language model associated with publication time, we can instead smooth using the temporal language model that is most likely to have generated the document. For each document, the temporal language model used for smoothing is selected using Kullback-Liebler divergence $KL(\theta_d || \theta_t)$.

\[
p_{BMN}(w \vert d) = \dfrac{n(w,D) + \mu \cdot \left[ (1-\alpha) \cdot p(w\vert \pi_t) + \alpha \cdot p(w \vert \theta_c)\right]}{n(D) + \mu}
\]

Where $\pi_t$ is the temporal language model with the smallest KL-divergence from the document language model:
\[
p(\pi_i \vert D) = f(KL(\pi_i \vert \theta_d))
\]

\subsection{Model 4: Smoothing using weighted average of all temporal language models}

Instead of smoothing by a single language model, following the approach used by \cite{Wei2006}, we can smooth the document using the weighed combination of all temporal language models.

\[
p_{TSA}(w \vert D)= \sum_{t=1}^{K} p(w \vert \theta_t) p(\theta_t \vert D)
\]

Where the $p(\theta_t \vert D) \sim KL(\theta_c \vert\vert \theta_t)$.


\subsection{Estimating temporal language models}

Adopting a unigram multinomial language model for $\theta_t$, the next step is to estimate $p(w \vert \theta_t)$.  
The simplest approach is to use the maximum likelihood estimator:

\[
p_{MLE} (w \vert \theta_t) = \dfrac{n(w, t)}{n(t)}
\]

Where $t$ is the temporal interval. In our experiments we found the MLE model to be too similar too 
the collection model, dampening the effect of the available temporal information. We explored two
different approaches to enhancing the temporal language model.  

First, we instead used the normalized pointwise mutual information:

\[
npmi(w;t) = \dfrac{\log \dfrac{p(w,t)}{p(w)p(t)}}{-\log[p(w,t)]}
\]

The resulting value ranges from -1 to 1. For estimating $p(w \vert t)$, all values less than zero were treated as 0:

\[
p_{NPMI} (w \vert t) = \dfrac{npmi(w;t)}{\sum_{w \in W} npmi(w; t)}
\]

Second, we used the likelihood ratio test to restrict the terms allowed in the model $\theta_t$. Setting $\alpha=0.01$, 
only terms $w$ found to be significantly associated with time $t$ are added to the model. This is the final approach
used in our reported results.

\section{Experiments and results}
\subsection{Data}

Experiments were conducted using seven TREC test collections. The first five collections were included for comparability to \cite{Liu2004} and \cite{Wei2006}. 
These include the Associated Press Newswire (AP) 1988-90 with queries 51-150; Financial Times (FT) 1991-94 with queries 301-400; 
Los Angeles Times (LA) with queries 301-400; San Jose Mercury News (SJM) 1991 with queries 51-150; 
and Wall Street Journal (WSJ) 1987-92 with queries 51-100 and 151-200. 
Additional experiments were conducted using the Tweets2011 test collection with queries 1-49 and 50-110.  
Queries were constructed using only the ``title'' field of the TREC topics. The standard relevance judgments were used. 
Queries with no documents in the judged pool for a collection were removed.  Information about each collection is presented in Table \ref{table.details}. 
All documents and queries are stemmed using the Krovetz \cite{} stemmer and stopwords removed.

 [Describe Tweets2011 corpus...] 

%\begin{table*}[t]
\begin{table}[htdp]
\small
\caption{Test collection details.  }
\begin{center}
\tabcolsep=0.11cm
\begin{tabular}{| l | l | l | l | }
\hline
\bf{Collection} & \bf{\# docs}  & \bf{Queries} & \bf{\# queries} \\ \hline
AP & 242,918 & 51-150 & 99 \\ \hline
FT & 210,158 & 301-400 & 95 \\ \hline
SJM & 90,257 & 51-150 & 94 \\ \hline
LA & 131,896 & 301-400 & 98 \\ \hline
WSJ & 173,252 & 51-100, 151-200 & 100 \\ \hline
Tweets2011 & 11,908,899 & 1-49  50-110  & 110\\ \hline
\end{tabular}
\end{center}
\label{table.details}
\normalsize
%\end{table*}
\end{table}

\subsection{Parameter estimation}

Following \cite{Liu2004} and \cite{Wei2006}, the AP collection is used as the training collection to estimate parameters that are then tested on the FT, LA, SJM, and WSJ collections. 
The Tweets2011 collection (queries 1-49) is used to estimate parameters that are then tested on queries 50-110. Models are optimized for mean average precision (MAP) using an exhaustive search.

\subsubsection{Temporal interval}

An important parameter in the definition of temporal language models is the temporal interval.  Using the AP and Tweets 2011 training collections, we explored hourly, daily, weekly, bi-weekly, and monthly intervals.
All documents with timestamps in each interval are candidates for inclusion in the associated temporal language model.

[Chart performance with different temporal intervals.]

\subsubsection{LDA-based language models}

For the LDA-based language models, we attempted to repeat the experiment described in \cite{Wei2006} using off-the-shelf software, specifically Mallet 2.0.7. The Mallet LDA implementation
has several parameters including the number of topics, number of sampling iterations, and burn-in. For the number of topics, we used 800 as identified by Wei and Croft. Because the implementations
differ significantly, we could use the number of iterations and chains specified in \cite{Wei2006}. We explored several combinations of iterations/burn-in including 500/100, the Mallet default 1000/200, and 5000/1000. 
We found the effectiveness and speed of the Mallet default values to be adequate for this study \footnote{We did find that increasing the number of iterations improved effectiveness, sometimes significantly. However, since 
LDA-based language models are not the focus of this study, we accepted the default values.}  We accept the original paper's values for the smoothing parameters $\lambda=0.07$ and $\mu=1000$.

\subsubsection{Temporal smoothing parameters}

For each of the temporal models, we must estimate the mixing parameter $\lambda$.  As noted above, the AP and Tweets2011 collections are used for training. Separate values are estimated for 
both feedback $\lambda_{RM3}$ and non-feedback $\lambda_{QL}$ scenarios.

Table \ref{table.params}

\begin{table}[htdp]
\small
\caption{Parameter estimates. }
\begin{center}
\begin{tabular}{| l | l | l |}
\hline
\bf{Model}& \bf{AP} 								&  \bf{Tweets2011}\\ \hline 
KDE 	& $\alpha_{QL} = 0.35, \alpha_{RM3} = 0.15$ 		& $\alpha_{QL} = 0.45, \alpha_{RM3} = 0.05$ \\
TSM 	& $\lambda_{QL} = 0.05, \lambda_{RM3} = 0.05$ 	& $\lambda_{QL} = 0.25, \lambda_{RM3} = 0.05$ \\ 
BMN 	& $\lambda_{QL} = 0.05, \lambda_{RM3} = 0.15$ 	& $\lambda_{QL} = 0.05, \lambda_{RM3} = 0.05$ \\ 
TSA 		& $\lambda_{QL} = 0.05, \lambda_{RM3} = 0.15$ 	& $\lambda_{QL} = 0.15, \lambda_{RM3} = 0.10$ \\ 
LDA 		& $\lambda = 0.7$							& $\lambda = 0.7 $ \\ \hline
\end{tabular}
\end{center}
\label{table.params}
\normalsize
\end{table}


\section{Results}

Results are presented in Table \ref{table.results.ql} and \ref{table.results.rm3} and \ref{table.posneg.ql} and \ref{table.posneg.rm3}.

\begin{table}[htdp]
\small
\caption{Symbols indicating statistically significant changes. }
\begin{center}
\begin{tabular}{l | l }
\hline
\bf{Symbol}		& \bf{Description}\\ \hline 
$\blacktriangle$		&	$p < 0.01$ improvement against the QL baseline \\
$\blacktriangledown$	&	$p < 0.01$ degradation against the QL baseline \\
$\vartriangle$			&	$p < 0.05$ improvement against the QL baseline \\
$\triangledown$		&	$p < 0.05$ degradation against the QL baseline \\
$\uparrow$			&	$p < 0.10$ improvement against the QL baseline \\
$\downarrow$			&	$p < 0.10$ degradation against the QL baseline \\ \hline
\end{tabular}
\end{center}
\label{table.pval}
\normalsize
\end{table}


\begin{table*}[t]
\small
\caption{Comparison of query likelihood (QL) with temporal models. }
\begin{center}
\tabcolsep=0.11cm
\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
\bf{Model}& \bf{AP} & \bf{FT} & \bf{LA} & \bf{SJM} & \bf{WSJ} & \bf{TWEETS2011} & \bf{TWEETS2012} \\ \hline 
KL & 0.1885  & 0.2571  & 0.2373  & 0.1951  & 0.2724  & 0.2503  & 0.2049 \\ \hline  \hline
KDE & 0.1894  & 0.2539 $^{\downarrow}$  & 0.2393  & 0.1939  & 0.2722  & 0.2623 $^{\blacktriangle}$  & 0.2146 $^{\blacktriangle}$ \\ \hline 
TSM & 0.1884  & 0.2563  & 0.2360 $^{\triangledown}$  & 0.1941 $^{\downarrow}$  & 0.2717 $^{\downarrow}$  & 0.2334  & 0.1580 $^{\blacktriangledown}$ \\ \hline 
BMN & 0.1883  & 0.2591  & 0.2388  & 0.1901 $^{\triangledown}$  & 0.2712  & 0.2000 $^{\blacktriangledown}$  & 0.1565 $^{\blacktriangledown}$ \\ \hline 
TSA & 0.1889  & 0.2589 $^{\blacktriangle}$  & 0.2395 $^{\vartriangle}$  & 0.1948  & 0.2726  & 0.2229 $^{\blacktriangledown}$  & 0.1692 $^{\blacktriangledown}$ \\ \hline
LDA & 0.2155 $^{\blacktriangle}$  & 0.2666  & 0.2572 $^{\uparrow}$  & 0.2157 $^{\blacktriangle}$  & 0.2977 $^{\blacktriangle}$  & 0.2436  & 0.1957 \\ \hline  
\end{tabular}
\end{center}
\label{table.results.ql}
\normalsize
\end{table*}


\begin{table*}[t]
\small
\caption{Comparison of relevance models (RM3) with temporal models. }
\begin{center}
\tabcolsep=0.11cm
\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
\bf{Model}& \bf{AP} & \bf{FT} & \bf{LA} & \bf{SJM} & \bf{WSJ} & \bf{TWEETS2011} & \bf{TWEETS2012} \\ \hline 
RM3 & 0.2098  & 0.2684  & 0.2485  & 0.2145  & 0.2975  & 0.2895  & 0.2406 \\ \hline  \hline
KDE & 0.2102  & 0.2680  & 0.2477  & 0.2137  & 0.2954 $^{\triangledown}$  & 0.2890  & 0.2427 \\ \hline 
TSM & 0.1884 $^{\blacktriangledown}$  & 0.2563 $^{\blacktriangledown}$  & 0.2360 $^{\blacktriangledown}$  & 0.1941 $^{\blacktriangledown}$  & 0.2717 $^{\blacktriangledown}$  & 0.2370 $^{\triangledown}$  & 0.1583 $^{\blacktriangledown}$ \\ \hline 
BMN & 0.2102  & 0.2717  & 0.2524 $^{\uparrow}$  & 0.2092 $^{\triangledown}$  & 0.2970  & 0.2167 $^{\blacktriangledown}$  & 0.1707 $^{\blacktriangledown}$ \\ \hline 
TSA & 0.2123 $^{\uparrow}$  & 0.2721 $^{\uparrow}$  & 0.2551 $^{\vartriangle}$  & 0.2172  & 0.2971  & 0.2448 $^{\blacktriangledown}$  & 0.1896 $^{\blacktriangledown}$ \\ \hline 
LDA & 0.2349 $^{\blacktriangle}$  & 0.2650  & 0.2721 $^{\uparrow}$  & 0.2299 $^{\vartriangle}$  & 0.3234 $^{\blacktriangle}$  & 0.2588 $^{\triangledown}$  & 0.2139 $^{\triangledown}$ \\ \hline 
\end{tabular}
\end{center}
\label{table.results.rm3}
\normalsize
\end{table*}

\begin{table*}[t]
\small
\caption{Number of queries with increased/decreased MAP over baseline QL. Values in parenthesis are number of queries with increased/decrease MAP > 0.05. }
\begin{center}
\tabcolsep=0.11cm
\begin{tabular}{l | l | l | l | l | l | l | l}
\hline
\bf{Model}& \bf{AP} & \bf{FT} & \bf{LATIMES} & \bf{SJM} & \bf{WSJ} & \bf{TWEETS2011} & \bf{TWEETS2012} \\ \hline 
KDE & 52/44 (0/0)  & 34/46 (1/3)  & 42/45 (2/1)  & 38/45 (0/0)  & 42/52 (1/0)  & 33/16 (5/1)  & 36/23 (5/1) \\ \hline 
TSM & 26/63 (1/0)  & 24/51 (0/0)  & 23/59 (0/0)  & 26/58 (0/0)  & 36/59 (0/0)  & 26/22 (10/12)  & 20/40 (4/21) \\ \hline 
BMN & 44/52 (1/0)  & 28/42 (2/0)  & 31/50 (3/0)  & 27/59 (1/6)  & 29/64 (1/0)  & 11/32 (0/18)  & 7/47 (1/23) \\ \hline 
TSA & 46/48 (0/0)  & 40/22 (1/0)  & 40/35 (0/0)  & 39/46 (0/0)  & 35/56 (0/0)  & 12/31 (2/12)  & 8/46 (1/17) \\ \hline 
\end{tabular}
\end{center}
\label{table.posneg.ql}
\normalsize
\end{table*}

\begin{table*}[t]
\small
\caption{Number of queries with increased/decreased MAP over baseline RM3. Values in parenthesis are number of queries with increased/decrease MAP > 0.05. }
\begin{center}
\tabcolsep=0.11cm
\begin{tabular}{l | l | l | l | l | l | l | l}
\hline
\bf{Model}& \bf{AP} & \bf{FT} & \bf{LATIMES} & \bf{SJM} & \bf{WSJ} & \bf{TWEETS2011} & \bf{TWEETS2012} \\ \hline 
KDE & 44/50 (0/0)  & 33/46 (2/0)  & 32/53 (0/0)  & 34/54 (1/0)  & 38/59 (0/0)  & 23/20 (0/1)  & 30/28 (2/0) \\ \hline 
TSM & 20/77 (0/12)  & 22/67 (3/11)  & 32/63 (3/13)  & 22/71 (2/17)  & 15/86 (1/20)  & 17/33 (5/20)  & 14/46 (3/29) \\ \hline 
BMN & 52/44 (0/0)  & 42/39 (2/2)  & 43/45 (4/1)  & 30/61 (1/5)  & 33/63 (1/0)  & 12/38 (3/23)  & 13/47 (2/31) \\ \hline 
TSA & 53/45 (2/1)  & 53/26 (2/1)  & 44/45 (7/2)  & 45/47 (5/4)  & 51/50 (0/1)  & 14/33 (1/17)  & 13/46 (1/24) \\ \hline 
\end{tabular}
\end{center}
\label{table.posneg.rm3}
\normalsize
\end{table*}



\subsection{Conclusion and future work}


\section{Discussion}

  
\bibliographystyle{abbrv} 
\bibliography{sigir2015}

\end{document}
